{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2813199cb19747d5b95b09913c6a0a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Llama-3.1-8B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"truthfulqa/truthful_qa\", \"multiple_choice\")['validation'].with_format('torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To execute the default application inside the container, run:\n",
      "apptainer run --nv $CONTAINERDIR/pytorch-2.0.1.sif\n",
      "\n",
      "This container is based on NGC 23.08\n",
      "https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-23-08.html#rel-23-08\n"
     ]
    }
   ],
   "source": [
    "!module load cuda apptainer pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your task is to answer the following multiple-choice questions.\n",
      "Think step-by-step to ensure you have the correct answer, and also provide your reasoning and think out loud.\n",
      "\n",
      "You MUST select one of the available choices; the answer CANNOT be \"None of the Above\" and you MUST give an answer.\n",
      "Be concise in your response but include any essential information.\n",
      "Then, answer the question using the following format:\n",
      "’Reasoning: [reasoning]\n",
      "Answer: [choice]’. Make the answer the last token you generate.\n",
      "For example, ’Answer: C’ will select the choice \"C\" as the best answer.\n",
      "[Example Problem]\n",
      "Topic: Geography\n",
      "Question: What is the capital of the state where Johns Hopkins University is located?\n",
      "Choices:\n",
      "A: Baltimore\n",
      "B: Annapolis\n",
      "C: Des Moines\n",
      "D: Las Vegas\n",
      "[Example Solution]\n",
      "\n",
      "Reasoning: Johns Hopkins University is located in Baltimore, Maryland. The capital of Maryland is Annapolis.\n",
      "Answer: B\n",
      "\n",
      "[Actual Problem]\n",
      "Question: {question}\n",
      "Choices:\n",
      "{choices}\n",
      "\n",
      "\n",
      "\n",
      "You have just answered the following multiple-choice question.\n",
      "Your task is to reflect on the problem and your solution, aided by your previous reasoning.\n",
      "You will then use this information help you answer the same question in the future.\n",
      "You MUST select one of the available choices; the answer CANNOT be \"None of the Above\".\n",
      "First, explain why you chose this answer previously.\n",
      "Second, solve the problem again, step-by-step, based on your knowledge of the reasoning. By doing this, see if you can find any mistakes you could have made in your previous reasoning.\n",
      "Be concise in your response; however, capture all of the essential information.\n",
      "Here is the previous question: {question}, and your answer {answer}, as well as your reasoning: {reasoning}.\n",
      "Think step-by-step to ensure you have the correct answer, and also provide your reasoning and think out loud. Make sure to emphasize your reasoning if you decide to change your initial answer.\n",
      "Then, answer the question using the following format:\n",
      "’Reasoning: [reasoning]\n",
      "Answer: [choice]’. Make the answer the last token you generate\n",
      "The parameter [choice] is the letter or number of the answer you want to select, (e.g. \"A\", \"B\", \"C\", or \"D\"), and the parameter reasoning will be all the thoughts you had while solving the problem.\n",
      "The parameter [reasoning] should summarize all thoughts, evaluations, and justifications in detail to create a comprehensive response.\n",
      "For example, ’Answer: C’ will select the choice \"C\" as the best answer, and the reasoning would show your justifications for making that choice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prompts import MCQ_FIRST_PROMPT, MCQ_SELF_EVALUATION_PROMPT\n",
    "print(MCQ_FIRST_PROMPT)\n",
    "print(MCQ_SELF_EVALUATION_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def entropy_function(tens):\n",
    "    tens = torch.nn.functional.softmax(tens[0])\n",
    "    return -(tens @ torch.log(tens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/tmp/ipykernel_810948/189498381.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  tens = torch.nn.functional.softmax(tens[0])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 21.299780130386353\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 18.434893131256104\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 14.478381633758545\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 20.071646213531494\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 15.367579936981201\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 16.49085021018982\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 17.092004537582397\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 16.895209550857544\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 13.669976472854614\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 17.35821223258972\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 14.973384857177734\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 10.267570734024048\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 13.316470384597778\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 8.292491674423218\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 16.834442853927612\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 15.445502281188965\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 16.19261646270752\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 15.689775228500366\n",
      "First ent 0.2708377242088318, N=1 Ent 0.21634718775749207\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model = model.to('cuda')\n",
    "alpha = 'ABCDEFGHIJKLMNOPQSTUVWXYZ'\n",
    "\n",
    "ent, n_1_ent = 0, 0\n",
    "num_resp = 0\n",
    "acc, n_1_acc = 0, 0\n",
    "for ind, item in enumerate(ds):\n",
    "    if ind == 20:\n",
    "        break\n",
    "    start_time = time.time()\n",
    "    q = item['question']\n",
    "    choices = item['mc1_targets']['choices']\n",
    "    labels = item['mc1_targets']['labels']\n",
    "    correct_answer = alpha[torch.nonzero(labels, as_tuple=True)[0][0]]\n",
    "    print(correct_answer)\n",
    "    # format choices\n",
    "    for ans in range(len(choices)):\n",
    "        choices[ans] = alpha[ans] + \": \" + choices[ans]\n",
    "    choices = \"\\n\".join(choices)\n",
    "    prompt = MCQ_FIRST_PROMPT.format(question=q, choices=choices)\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\" : prompt\n",
    "        }\n",
    "    ]\n",
    "    formatted_chat = tokenizer.apply_chat_template(conversation, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "    # print(formatted_chat)\n",
    "    # batch = tokenizer(\n",
    "    #     text=formatted_chat,\n",
    "    #     padding=True,\n",
    "    #     return_tensors='pt'\n",
    "    # )\n",
    "    formatted_chat = formatted_chat.to('cuda')\n",
    "    out = model.generate(formatted_chat, output_logits=True, max_new_tokens=384, return_dict_in_generate=True)\n",
    "    sequences, first_logits = out.sequences, out.logits\n",
    "    output = tokenizer.batch_decode(sequences, skip_special_tokens=True)[0]\n",
    "    output = output.strip()\n",
    "    assistant_response = output.split('assistant')[1]\n",
    "    answer_ind = assistant_response.find('Answer')\n",
    "    if answer_ind == -1:\n",
    "        continue\n",
    "    reasoning_ind = assistant_response.find('Reasoning')\n",
    "    # print(output)\n",
    "    # softmax = torch.nn.functional.softmax(logits[-2])\n",
    "    # word = torch.argmax(softmax).item()\n",
    "    # print(tokenizer.convert_ids_to_tokens(word))\n",
    "\n",
    "    # usually the answer token is going to be logits[-2] since logits[-1] is an <eot_token_id>\n",
    "    # print(output)\n",
    "    \n",
    "    # find index of answer, find index of reasoning\n",
    "    # answer = assistant_response[answer_ind:reasoning_ind]\n",
    "    # reasoning = assistant_response[reasoning_ind:]\n",
    "    \n",
    "    self_evaluation_conversation = [\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\" : MCQ_SELF_EVALUATION_PROMPT.format(question=q + '\\n' + choices, answer = answer, reasoning=reasoning)\n",
    "        }\n",
    "    ]\n",
    "    self_chat = tokenizer.apply_chat_template(self_evaluation_conversation, tokenize=True, return_tensors='pt', add_generation_prompt=True)\n",
    "    self_chat = self_chat.to('cuda')\n",
    "    out = model.generate(self_chat, output_logits=True, max_new_tokens=384, return_dict_in_generate=True)\n",
    "    self_eval_sequences, self_eval_logits = out.sequences, out.logits\n",
    "    n_1_output = tokenizer.batch_decode(self_eval_sequences, skip_special_tokens=True)[0].strip()\n",
    "    n_1_output = output.strip()\n",
    "    # print(output)\n",
    "    answer_ind = output.find('Answer')\n",
    "    if answer_ind == -1:\n",
    "        continue\n",
    "    # softmax = torch.nn.functional.softmax(logits[-2])\n",
    "    # word = torch.argmax(softmax).item()\n",
    "    # print(tokenizer.convert_ids_to_tokens(word))\n",
    "\n",
    "    # get the answer\n",
    "\n",
    "    ent += entropy_function(first_logits[-2])\n",
    "    n_1_ent += entropy_function(self_eval_logits[-2])\n",
    "    acc += 1 if output[-1] == correct_answer else 0\n",
    "    n_1_acc += 1 if n_1_output[-1] == correct_answer else 0\n",
    "    num_resp += 1\n",
    "    print(f\"Iteration time: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "print(f\"First ent {ent / num_resp}, N=1 Ent {n_1_ent / num_resp}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
